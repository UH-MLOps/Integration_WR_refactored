{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accb32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from geopy.distance import distance, geodesic\n",
    "from geopy import Point, distance as geo_distance\n",
    "from geographiclib.geodesic import Geodesic as GD\n",
    "from scipy.interpolate import interpn\n",
    "import folium\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc38afb",
   "metadata": {},
   "source": [
    "## steps\n",
    "1. read in files (json or csv) and parse the data into dataframe\n",
    "2. check if timestamp is in accending order\n",
    "3. longitude convertion: western negative to 0-360 format\n",
    "4. according to the current position and timestamp, filter out the needed waypoints to df_filter\n",
    "5. create section_waypoints\n",
    "6. generate interpolated coordinates, timestamps and write cog\n",
    "7. interpolate corresponding weather attributes for all the spatial points in the experimenting area\n",
    "8. do the transformation for weather attributes (difference between previous forecasted and latest forecasted)\n",
    "9. zero padding\n",
    "10. pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b431972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longitude list and latitude list of the grids in the exaperimenting area\n",
    "long_list = np.array([156.00000694, 157.25000699, 158.50000705, 159.7500071 ,\n",
    "                      161.00000716, 162.25000722, 163.50000727, 164.75000733,\n",
    "                      166.00000738, 167.25000744, 168.50000749, 169.75000755,\n",
    "                      171.00000761, 172.25000766, 173.50000772, 174.75000777,\n",
    "                      176.00000783, 177.25000788, 178.50000794, 179.75000799,\n",
    "                      181.00000805, 182.25000811, 183.50000816, 184.75000822,\n",
    "                      186.00000827, 187.25000833, 188.50000838, 189.75000844,\n",
    "                      191.00000849, 192.25000855, 193.50000861, 194.75000866,\n",
    "                      196.00000872, 197.25000877, 198.50000883, 199.75000888,\n",
    "                      201.00000894, 202.250009  , 203.50000905, 204.75000911,\n",
    "                      206.00000916, 207.25000922, 208.50000927, 209.75000933,\n",
    "                      211.00000938, 212.25000944, 213.5000095 , 214.75000955,\n",
    "                      216.00000961, 217.25000966, 218.50000972, 219.75000977,\n",
    "                      221.00000983, 222.25000988, 223.50000994, 224.75001   ,\n",
    "                      226.00001005, 227.25001011, 228.50001016])\n",
    "lat_list = np.array([50.  , 49.75, 49.5 , 49.25, 49.  , 48.75, 48.5 , 48.25, 48.  ,\n",
    "       47.75, 47.5 , 47.25, 47.  , 46.75, 46.5 , 46.25, 46.  , 45.75,\n",
    "       45.5 , 45.25, 45.  , 44.75, 44.5 , 44.25, 44.  , 43.75, 43.5 ,\n",
    "       43.25, 43.  , 42.75, 42.5 , 42.25, 42.  , 41.75, 41.5 , 41.25,\n",
    "       41.  , 40.75, 40.5 , 40.25, 40.  , 39.75, 39.5 , 39.25, 39.  ,\n",
    "       38.75, 38.5 , 38.25, 38.  , 37.75, 37.5 , 37.25, 37.  , 36.75,\n",
    "       36.5 , 36.25, 36.  , 35.75, 35.5 , 35.25, 35.  , 34.75, 34.5 ,\n",
    "       34.25, 34.  , 33.75, 33.5 , 33.25, 33.  , 32.75, 32.5 , 32.25,\n",
    "       32.  , 31.75, 31.5 , 31.25, 31.  , 30.75, 30.5 , 30.25, 30.  ,\n",
    "       29.75, 29.5 , 29.25, 29.  , 28.75, 28.5 , 28.25, 28.  , 27.75,\n",
    "       27.5 , 27.25, 27.  , 26.75, 26.5 , 26.25, 26.  , 25.75, 25.5 ,\n",
    "       25.25])\n",
    "\n",
    "range_ul = [50, 156.00000694]  # upper left corner coordinates of the experimenting area\n",
    "range_lr = [25, 228.50001016]  # lower right corner coordinates of the experimenting area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c029210",
   "metadata": {},
   "source": [
    "### 1. load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c43384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: load the JSON file\n",
    "# the provided example is not in the northen pacific area\n",
    "with open('input_sample.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# create a dictionary to store the data\n",
    "parsed_data = {'latitude': [], 'longitude': [], 'timestamp': []}\n",
    "\n",
    "# iterate over each coordinate and timestamp, and store them in the dictionary\n",
    "for i in range(len(data['coordinates'])):\n",
    "    parsed_data['latitude'].append(data['coordinates'][i]['latitude'])\n",
    "    parsed_data['longitude'].append(data['coordinates'][i]['longitude'])\n",
    "    parsed_data['timestamp'].append(data['timestamps'][i])\n",
    "\n",
    "# create a Pandas DataFrame from the parsed data\n",
    "input_df = pd.DataFrame(parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f30fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: read the csv file into a DataFrame\n",
    "input_df = pd.read_csv('98765434_voyage-export.csv')\n",
    "\n",
    "# extract the desired variables\n",
    "variables = ['LATITUDE', 'LONGITUDE', 'TIME AT WAYPOINT']\n",
    "input_df = input_df[variables]\n",
    "input_df = input_df.rename(columns={'TIME AT WAYPOINT': 'timestamp', 'LATITUDE': 'latitude', 'LONGITUDE': 'longitude'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051f9e1",
   "metadata": {},
   "source": [
    "### 2 + 3 timestamp and longitude conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f03b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert longitudes in the western hemisphere to positive numbers\n",
    "input_df.loc[input_df['longitude'] < 0, 'longitude'] += 360\n",
    "# convert timestamp\n",
    "input_df['timestamp'] = pd.to_datetime(input_df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "if not input_df['timestamp'].is_monotonic_increasing:\n",
    "    # Sort the DataFrame by the timestamp column\n",
    "    input_df = input_df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9748e6e",
   "metadata": {},
   "source": [
    "### 4. filter needed waypoints according to current T0 and current position G0 (lat0, lon0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffe73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current T0 and current position G0 (lat0, lon0) should be provided by json file\n",
    "# Now define T0 and G0 as examples\n",
    "\n",
    "# example 1: starting point is where the ship not yet entered the experimenting area\n",
    "T0_str = '2022-10-09 17:05:13+00:00'\n",
    "G0 = (32,124)\n",
    "T0 = pd.Timestamp(T0_str, tz='UTC')\n",
    "\n",
    "# example 2: starting point is where the ship is in the middle of the experimenting area\n",
    "#T0_str = '2022-10-14 16:28:15+00:00'\n",
    "#G0 = (49,171)\n",
    "#T0 = pd.Timestamp(T0_str, tz='UTC')\n",
    "\n",
    "# example 3: starting point is where the ship has already sailed outside the experimenting area\n",
    "#T0_str = '2022-10-21 07:07:33+00:00'\n",
    "#G0 = (34.313492,239.532944)\n",
    "#T0 = pd.Timestamp(T0_str, tz='UTC')\n",
    "           \n",
    "# filter the dataframe by timestamp and longitude\n",
    "mask = (input_df['timestamp'] >= T0) & (input_df['longitude'].between(range_ul[1], range_lr[1]))\n",
    "\n",
    "# check if the current point or timestamp is after/ exceed the experimenting area\n",
    "if mask.any():\n",
    "    # do the filtering\n",
    "    index_first = np.nonzero(mask.values)[0][0]\n",
    "    index_last = np.nonzero(mask.values)[0][-1]\n",
    "    df_filtered = input_df[index_first-1:index_last+2]\n",
    "else:\n",
    "    # Report\n",
    "    print('Error: rerouting point outside the right bound of the experimenting area')\n",
    "    # also need to stop excuting at all\n",
    "\n",
    "'''in case of the 3nd senario, quit from here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c5e1e",
   "metadata": {},
   "source": [
    "### 5. waypoints_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3964aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n",
      "/var/folders/fp/9nc54f7n0z566skqx52tb5340000gp/T/ipykernel_48362/37114126.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sections_waypoints = sections_waypoints.append({\n"
     ]
    }
   ],
   "source": [
    "# store the information between each pair of consecutive waypoints from df_filtered in sections_waypoints\n",
    "\n",
    "# calculating bearing\n",
    "def get_bearing(lat1, lat2, long1, long2):\n",
    "    '''The function is used to calculate the CoG from node 1 to node 2'''\n",
    "    brng = GD.WGS84.Inverse(lat1, long1, lat2, long2)['azi1']\n",
    "    if brng < 0:\n",
    "        brng = brng + 360\n",
    "    return brng\n",
    "\n",
    "\n",
    "sections_waypoints = pd.DataFrame(columns=['E_latitude', 'E_longitude', 'L_latitude', \n",
    "                                                'L_longitude', 'E_timestamp', 'L_timestamp', 'cog'])\n",
    "\n",
    "for i in range(len(df_filtered) - 1):\n",
    "    row1 = df_filtered.iloc[i]\n",
    "    row2 = df_filtered.iloc[i + 1]\n",
    "\n",
    "    # Calculate course over ground between two waypoints\n",
    "    cog = get_bearing(row1['latitude'], row2['latitude'], row1['longitude'], row2['longitude'])\n",
    "\n",
    "    # Add the information to the sections_waypoints dataframe\n",
    "    sections_waypoints = sections_waypoints.append({\n",
    "            'E_latitude': row1['latitude'],\n",
    "            'E_longitude': row1['longitude'],\n",
    "            'E_timestamp': row1['timestamp'],\n",
    "            'L_latitude': row2['latitude'],\n",
    "            'L_longitude': row2['longitude'],\n",
    "            'L_timestamp': row2['timestamp'],\n",
    "            'cog': cog\n",
    "    }, ignore_index=True)\n",
    "sections_waypoints['E_latitude'] = sections_waypoints['E_latitude'].astype(float)\n",
    "sections_waypoints['E_longitude'] = sections_waypoints['E_longitude'].astype(float)\n",
    "sections_waypoints['L_latitude'] = sections_waypoints['L_latitude'].astype(float)\n",
    "sections_waypoints['L_longitude'] = sections_waypoints['L_longitude'].astype(float)\n",
    "sections_waypoints['E_timestamp'] = pd.to_datetime(sections_waypoints['E_timestamp'])\n",
    "sections_waypoints['L_timestamp'] = pd.to_datetime(sections_waypoints['L_timestamp'])\n",
    "sections_waypoints['cog'] = sections_waypoints['cog'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699eebb",
   "metadata": {},
   "source": [
    "### 6. generate interpolated coordinates, timestamp and write cog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799096ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty pandas DataFrame with column 'longitude'\n",
    "interpolation_full = pd.DataFrame({'longitude': long_list})\n",
    "\n",
    "# Add empty columns to the DataFrame\n",
    "interpolation_full['latitude'] = np.nan    # longitudes are given according to the grids\n",
    "                                           # latitudes are interpolated\n",
    "interpolation_full['cog'] = np.nan         \n",
    "interpolation_full['timestamp'] = pd.NaT\n",
    "interpolation_full['arriving_time_latest'] = np.nan\n",
    "interpolation_full['arriving_time_previous'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8197bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intp_lat_ts_cog(row):\n",
    "    \"\"\"\n",
    "    Finds the row in `sections_waypoints` where the logitudes in the grid `t_lon` falls between\n",
    "    the `E_longitude` and `L_longitude` values. Calculates the latitude\n",
    "    and timestamp for the target point T_point and sets the `latitude`,\n",
    "    `timestamp`, and `cog` columns in accordingly.\n",
    "\n",
    "    Parameters:\n",
    "    t_lon (float): The longitude value to search for in `sections_waypoints`.\n",
    "    sections_waypoints (pd.DataFrame): The DataFrame containing the sections and waypoints.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: a row with interpolated values of latitude, timestamp and cog\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Get t_lon value \n",
    "    t_lon = row['longitude']\n",
    "\n",
    "    # Find row in `sections_waypoints` where `t_lon` falls between\n",
    "    #         the `E_longitude` and `L_longitude` values\n",
    "    mask = (sections_waypoints['E_longitude'] <= t_lon) & (t_lon <= sections_waypoints['L_longitude'])\n",
    "    row_sw = sections_waypoints.loc[mask]\n",
    "\n",
    "    # Calculate t_lat using linear interpolation\n",
    "    point1 = np.array([row_sw['E_latitude'], row_sw['E_longitude']])\n",
    "    point2 = np.array([row_sw['L_latitude'], row_sw['L_longitude']])\n",
    "    t_lat = point1[0] + ((t_lon - point1[1]) / (point2[1] - point1[1])) * (point2[0] - point1[0])\n",
    "\n",
    "    # Calculate t_timestamp using linear interpolation\n",
    "    e_ts = row_sw['E_timestamp']\n",
    "    l_ts = row_sw['L_timestamp']\n",
    "\n",
    "    t_ts = e_ts + ((t_lon - point1[1])[0] / (point2[1] - point1[1])[0]) * (l_ts - e_ts)\n",
    "\n",
    "    return pd.Series({'latitude': t_lat[0], 'timestamp': t_ts.values[0], 'cog': row_sw['cog'].iloc[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2626da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the index of the smallest longitude in grids that is larger than the current position G0[1]\n",
    "s_grids_lon_index = (interpolation_full['longitude'] >= G0[1]).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0440e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#interpolate latitude timestamp and write cog\n",
    "interpolation_full.loc[s_grids_lon_index:, ['latitude', 'timestamp', 'cog']] = (interpolation_full.loc[s_grids_lon_index:,:]\n",
    "                                                                                .apply(intp_lat_ts_cog, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efe7c1",
   "metadata": {},
   "source": [
    "### 7. read and interpolate corresponding weather attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98eba959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the lastest available weather forecast according to the current timepoint\n",
    "\n",
    "def get_last_weather_forecast(timestamp_str):\n",
    "    \n",
    "    '''return the forecast files names and the timestamps of the forecast'''\n",
    "    # only keep the year month day hour minute seconds\n",
    "    truncated_timestamp_str = timestamp_str[:19]\n",
    "    \n",
    "    dt = datetime.strptime(truncated_timestamp_str, '%Y-%m-%d %H:%M:%S')\n",
    "    forecast_hour = (dt.hour // 6) * 6\n",
    "    last_forecast = pytz.utc.localize(datetime(dt.year, dt.month, dt.day, forecast_hour))\n",
    "    previous_forecast = last_forecast-timedelta(hours = 6)\n",
    "    \n",
    "    # the most recent forecast and the previous one\n",
    "    last_forecast_name = (str(last_forecast.year)+str(last_forecast.month).zfill(2)+str(last_forecast.day).zfill(2)\n",
    "                             +str(last_forecast.hour).zfill(2))\n",
    "    \n",
    "    previous_forecast_name = (str(previous_forecast.year)+str(previous_forecast.month).zfill(2)\n",
    "                              +str(previous_forecast.day).zfill(2)+str(previous_forecast.hour).zfill(2))\n",
    "    return (last_forecast_name, previous_forecast_name),(last_forecast, previous_forecast) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cc5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name, forecast_ts = get_last_weather_forecast(T0_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67359252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the time difference in hours \n",
    "# between the arriving time at a location and the starting time of the weather forecast\n",
    "time_diff0 = lambda x: (pytz.utc.localize(x['timestamp']) - forecast_ts[0]) / timedelta(hours=1)\n",
    "time_diff1 = lambda x: (pytz.utc.localize(x['timestamp']) - forecast_ts[1]) / timedelta(hours=1)\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "interpolation_full.loc[s_grids_lon_index:, 'arriving_time_latest'] = (interpolation_full.loc[s_grids_lon_index:,:]\n",
    "                                                                      .apply(time_diff0, axis=1))\n",
    "interpolation_full.loc[s_grids_lon_index:, 'arriving_time_previous'] = (interpolation_full.loc[s_grids_lon_index:,:]\n",
    "                                                                      .apply(time_diff1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7caa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour2timeframe(row):\n",
    "    '''The timeframe is from 0 to 209, but the real hour is from 0 to 384. \n",
    "    From hour 120 onwards, every three hours per timeframe.\n",
    "    The function is to find out the positions in the timeframes according to the hour'''\n",
    "    if row['arriving_time_latest'] > 120:\n",
    "        tf_latest = (row['arriving_time_latest']-120)/3 + 120\n",
    "    else:\n",
    "        tf_latest = row['arriving_time_latest']\n",
    "        \n",
    "    if row['arriving_time_previous'] > 120:\n",
    "        tf_previous = (row['arriving_time_previous']-120)/3 + 120\n",
    "    else:\n",
    "        tf_previous = row['arriving_time_previous']\n",
    "    return pd.Series({'tf_latest': tf_latest, 'tf_previous': tf_previous})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e35d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_full.loc[s_grids_lon_index:, ['tf_latest', 'tf_previous']] = (interpolation_full.loc[s_grids_lon_index:,:]\n",
    "                                                                      .apply(hour2timeframe, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145f3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the needed weather forecast file\n",
    "wf_latest = np.load('/gfs_NP_'+forecast_name[0]+'.npy')\n",
    "wf_previous = np.load('/gfs_NP_'+forecast_name[1]+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bec7edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flip for the 1st dimension (latitude) because interpolation requires first dimension in accending order\n",
    "# slice and drop the last longitude as weather attributes are not needed for the destination\n",
    "wf_latest = np.flip(wf_latest, axis = 0)[:,:-1,:,:]\n",
    "wf_previous = np.flip(wf_previous, axis = 0)[:,:-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a819d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_row(row, wf, tsn):\n",
    "    \n",
    "    '''\n",
    "    The function aims to retrieve and interpolate the weather conditions at the given position according \n",
    "    to its spatial coordinates and arriving time (hour)\n",
    "    input:\n",
    "    wf is the weather forecast array, wf_latest or wf_previous\n",
    "    tsn = 'tf_latest' or 'tf_previous' according to wf\n",
    "    output: weather conditions for wind, wave and swell (8)\n",
    "    '''\n",
    "    # Define the interpolation points as a tuple of arrays\n",
    "    interp_points = (row['latitude'], row['longitude'], row[tsn], range(wf.shape[3]))\n",
    "\n",
    "    # Interpolate the weather variable values at the interpolation points\n",
    "    var_values = interpn((np.flip(lat_list), long_list, range(wf.shape[2]), range(wf.shape[3])),\n",
    "                     wf, interp_points, method='linear')\n",
    "    return var_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9635cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation of weather conditions forecasted in the latest weather forecast and the previous weather forecast\n",
    "interpolation_full.loc[s_grids_lon_index:,'latest_weather'] = interpolation_full.loc[s_grids_lon_index:,:].apply(lambda row: \n",
    "                                                                                                                 interp_row(row, wf_latest, 'tf_latest'), axis = 1)\n",
    "interpolation_full.loc[s_grids_lon_index:,'previous_weather'] = interpolation_full.loc[s_grids_lon_index:,:].apply(lambda row: \n",
    "                                                                                                                   interp_row(row, wf_previous, 'tf_previous'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4a15687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that converts a 1D numpy array of weather variables to the desired format\n",
    "def convert_weather_array(weather_array, cog):\n",
    "    # Make a copy of the input array of weather conditions\n",
    "    converted_array = np.copy(weather_array)\n",
    "\n",
    "    # Define the indices of the directions of wind, wave and swell\n",
    "    direction_indices = [1, 4, 7]\n",
    "\n",
    "    # Subtract the cog from the direction and take the absolute value and convert if larger than 180 degree\n",
    "    for idx in direction_indices:\n",
    "        diff = abs(weather_array[idx] - cog)\n",
    "        if diff <= 180:\n",
    "            converted_array[idx] = diff\n",
    "        else:\n",
    "            converted_array[idx] = 360 - diff\n",
    "\n",
    "    return converted_array\n",
    "\n",
    "\n",
    "# Apply the 'convert_weather_array' function to each row of the DataFrame\n",
    "converted_weather = interpolation_full.loc[s_grids_lon_index:,:].apply(lambda row: convert_weather_array(\n",
    "    row['latest_weather'], row['cog']) - convert_weather_array(row['previous_weather'], row['cog']), axis=1)\n",
    "\n",
    "# Concatenate the converted weather arrays into a single flattened array\n",
    "flattened_weather = np.concatenate(converted_weather.to_numpy())\n",
    "\n",
    "# Zero-pad the flattened array if necessary\n",
    "if len(flattened_weather) < 59 * 8:\n",
    "    padding = np.zeros((59 * 8 - len(flattened_weather),))\n",
    "    flattened_weather = np.concatenate((padding, flattened_weather))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ef19a",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92f44a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture:\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(472, 640) \n",
    "        self.layer_2 = nn.Linear(640, 120)\n",
    "        self.layer_3 = nn.Linear(120, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(640)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(120)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.kaiming_normal_(self.layer_1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.layer_2.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.layer_3.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.layer_out.weight)      \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "model = BinaryClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ac5e7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trained model\n",
    "trained_model = torch.load('pretrained_wr_v2.pt')['state_dict']\n",
    "model.load_state_dict(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d54b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerouting is not recommended!\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "tensor_input = torch.tensor(flattened_weather).float()\n",
    "tensor_input = tensor_input.reshape(1, -1)\n",
    "model.eval()\n",
    "probs = torch.sigmoid(model.forward(tensor_input))\n",
    "preds = (probs >= 0.5).int()\n",
    "print('Rerouting is recommended!' if preds == 1 else 'Rerouting is not recommended!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85879a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbedfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb6376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
